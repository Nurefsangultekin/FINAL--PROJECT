import os
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from langchain.text_splitter import CharacterTextSplitter
import tiktoken

def count_tokens(text, model_name="text-embedding-3-small"):
    enc = tiktoken.encoding_for_model(model_name)
    return len(enc.encode(text))

def create_embeddings_from_csv(file_path, id_col, text_col):
    print(f"üîç ƒ∞≈üleniyor: {file_path}")
    try:
        df = pd.read_csv(file_path)
        if text_col not in df.columns:
            print(f"‚ö†Ô∏è '{text_col}' s√ºtunu {file_path} i√ßinde bulunamadƒ±, atlanƒ±yor.")
            return []
        docs = [
            Document(page_content=row[text_col], metadata={"id": row[id_col]})
            for _, row in df.iterrows()
        ]
        return docs
    except Exception as e:
        print(f"‚ùå Hata olu≈ütu: {e}")
        return []

# CSV dosyalarƒ± ve ilgili s√ºtun adlarƒ±
csv_files = {
    "acik_riza_metni": ("acik_riza_metni_taslaklari_idli.csv", "ID", "A√ßƒ±k Rƒ±za Metni Taslaƒüƒ±"),
    "aydinlatma_metni": ("aydinlatma_metni_taslaklari_idli_v2.csv", "ID", "Aydƒ±nlatma Metni Taslaƒüƒ±"),
    "kvkk_onay_metni": ("kvkk_onay_muvafakatname_taslaklari_idli.csv", "ID", "KVKK Onay Metni Taslaƒüƒ±"),
    "kvkk_muvafakatname": ("kvkk_onay_muvafakatname_taslaklari_idli.csv", "ID", "KVKK Muvafakatname Taslaƒüƒ±")
}

embedding_model = OpenAIEmbeddings()
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
os.makedirs("legal_vector_index", exist_ok=True)

for file_key, (file_path, id_col, text_col) in csv_files.items():
    all_docs = create_embeddings_from_csv(file_path, id_col, text_col)
    split_docs = text_splitter.split_documents(all_docs)
    print(f"üìÑ {file_key}: Toplam belge sayƒ±sƒ± (par√ßalanmƒ±≈ü): {len(split_docs)}")

    # Batch i≈üle
    batch = []
    total_tokens = 0
    max_tokens = 280000
    vector_db = None

    for doc in split_docs:
        tokens = count_tokens(doc.page_content)
        if total_tokens + tokens > max_tokens:
            print(f"‚û°Ô∏è Yeni batch ba≈ülatƒ±lƒ±yor (toplam token: {total_tokens})")
            if vector_db is None:
                vector_db = FAISS.from_documents(batch, embedding_model)
            else:
                vector_db.add_documents(batch)
            batch = []
            total_tokens = 0
        batch.append(doc)
        total_tokens += tokens

    # Son batch‚Äôi kaydet
    if batch:
        print(f"‚úÖ Son batch i≈üleniyor (toplam token: {total_tokens})")
        if vector_db is None:
            vector_db = FAISS.from_documents(batch, embedding_model)
        else:
            vector_db.add_documents(batch)

    # Kayƒ±t
    index_path = f"legal_vector_index/{file_key}_vector_index"
    vector_db.save_local(index_path)
    print(f"‚úÖ {file_key} i√ßin embedding ba≈üarƒ±yla kaydedildi: {index_path}")
